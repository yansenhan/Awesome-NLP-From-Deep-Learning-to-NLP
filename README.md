# Awesome NLP: From Dummies To Experts
> This repo aims to record papers and materials on NLP.

# Table of Contents for Learning Materials
- [Learning Materials](#learning-materials)
  * [Neural Network Structures](#neural-network-structures)
  * [Neural Network Tricks](#neural-network-tricks)
  * [Word Represetations](#word-represetations)
  * [Sentence Representations](#sentence-representations)
  * [Sentence Matching](#sentence-matching)
  * [Documents or Information Retrieval](#documents-or-information-retrieval)
  * [Question Answering](#question-answering)
    + [Knowledge-Graph QA](#knowledge-graph-qa)
    + [Text-QA](#text-qa)

# Learning Materials
## Neural Network Structures
| Papers  | Codes  |
|---|---|
| [The Reversible Residual Network: Backpropagation Without Storing Activations](https://arxiv.org/pdf/1707.04585.pdf) |  |
| [An Empirical Exploration of Recurrent Network Architectures (2015)](http://proceedings.mlr.press/v37/jozefowicz15.pdf?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=revue)  |   |
| [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling (2014)](https://arxiv.org/pdf/1412.3555.pdf?ref=hackernoon.com)  |   |
| [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation (2014)](https://arxiv.org/pdf/1406.1078v3.pdf)  |   |

## Neural Network Tricks
| Papers  | Codes  |
|---|---|
| Dropout Seires |  |
| Dropout: [A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) |  |
| DropConnect: [Regularization of Neural Networks using DropConnect](http://yann.lecun.com/exdb/publis/pdf/wan-icml-13.pdf) |  |
| Spatial Dropout: [Efficient Object Localization Using Convolutional Networks](https://arxiv.org/pdf/1411.4280.pdf) |  |

## Word Represetations
| Papers  | Codes  |
|---|---|
| [Evaluating the Stability of Embedding-based Word Similarities]()  |   |
| [The Strange Geometry of Skip-gram with Negative Sampling]()  |   |
| [Factors Influencing the Surprising Instability of Word Embeddings]()  |   |

## Sentence Representations
| Papers  | Read  |
|---|---|
| [Pre-trained Models for Natural Language Processing: A Survey (2020)](https://arxiv.org/pdf/2003.08271.pdf)||
| [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators (2020)](https://openreview.net/pdf?id=r1xMH1BtvB) | √ |
| [Reformer: The Efficient Transformer](https://arxiv.org/pdf/2001.04451.pdf)| √ |
| [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2019)](https://arxiv.org/pdf/1810.04805.pdf) | √ |
| [Language Models are Unsupervised Multitask Learners (2019)](https://www.ceid.upatras.gr/webpages/faculty/zaro/teaching/alg-ds/PRESENTATIONS/PAPERS/2019-Radford-et-al_Language-Models-Are-Unsupervised-Multitask-%20Learners.pdf)| √ |
| [Deep contextualized word representations (2018)](https://arxiv.org/pdf/1802.05365.pdf) | √ |
| [Attention Is All You Need (2017)](https://arxiv.org/pdf/1706.03762.pdf)  | √ |

## Sentence Matching
| Papers  | Read  |
|---|---|
| [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (2019)](https://arxiv.org/pdf/1908.10084.pdf)||

## Documents or Information Retrieval
| Papers  | Read |
|---|---|
| Elastic Search: [Blog1](https://hackpython.com/blog/2019/06/28/Python%E8%BF%9B%E9%98%B6%E5%BF%85%E5%AD%A6%E5%BA%93%EF%BC%9Aelasticsearch-py%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3-%E4%B8%8A%E7%AF%87/), [Blog2](https://hackpython.com/blog/2019/06/29/Python%E8%BF%9B%E9%98%B6%E5%BF%85%E5%AD%A6%E5%BA%93%EF%BC%9Aelasticsearch-py%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3-%E4%B8%8B%E7%AF%87/), [Python Doc](https://elasticsearch-py.readthedocs.io/en/master/index.html), [Doc](https://www.elastic.co/guide/cn/elasticsearch/guide/current/index.html) | |
| [[Not paper]BERT在美团搜索核心排序的探索和实践 (2020)](https://tech.meituan.com/2020/07/09/bert-in-meituan-search.html)| √ |
| [Context-Aware Answer Sentence Selection With Hierarchical Gated Recurrent Neural Networks (2017)](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8226822&casa_token=jSg8SXOAUb0AAAAA:TlhlXshGkVeEdf3JoOc-wZ_JCTevb0uKpAdK1Rrq4hCJsvf-E8l_J5mcRFOd4cxR2XlGLWJsnw&tag=1) ||

## Model Distilling and Compression
| Papers  | Codes  |
|---|---|
| [Distilling Task-Specific Knowledge from BERT into Simple Neural Networks](https://arxiv.org/pdf/1903.12136.pdf?source=post_page---------------------------)| [Code](https://github.com/qiangsiwei/bert_distill) |
| [Large Batch Optimization for Deep Learning: Training BERT in 76 minutes](https://arxiv.org/pdf/1904.00962.pdf) |  |

## Question Answering

### Knowledge-Graph QA
| Papers  | Read  |
|---|---|
| [Embedding-based Retrieval in Facebook Search (2020)](https://arxiv.org/pdf/2006.11632.pdf) ||
| [Enhancing Key-Value Memory Neural Networks for Knowledge Based Question Answering (2019)](https://www.aclweb.org/anthology/N19-1301.pdf)||
| [Knowledge Graph Embedding Based Question Answering (2019)](https://dl.acm.org/doi/pdf/10.1145/3289600.3290956?casa_token=c6ZUr0Biz0YAAAAA:7VgKnEDVdYCxDUj6KtmwBnc0U9ZbH5q64xbIhiuxm8b0UBHmbJVr6gsTalqr6tH9teGITV09f6Rhjg) | √ |
| [Bidirectional Attentive Memory Networks for Question Answering over Knowledge Bases (2019)](https://arxiv.org/pdf/1903.02188.pdf) | √ |
| [Question Answering with Knowledge Base, Web and Beyond (2017)](https://link.springer.com/content/pdf/10.1007/s10844-019-00584-7.pdf) |  |
| [Automated Template Generation for Question Answering over Knowledge Graphs (2017)](https://dl.acm.org/doi/pdf/10.1145/3038912.3052583?casa_token=Ha_4RZHJ54cAAAAA:J0RFYmLvrWNa2ZCnHn4rnRQ1xZwpBLF9siKfyGG43Nh1TFhkmthIn2uAEB9cqL14UA2nwl4KDcJzfMU)||
| [Question Answering on Freebase via Relation Extraction and Textual Evidence (2016)](https://arxiv.org/pdf/1603.00957.pdf)||


### Text-QA
| Papers  | Read  |
|---|---|
| [Retrospective Reader for Machine Reading Comprehension (2020)](https://arxiv.org/pdf/2001.09694v2.pdf)| |
| [UnifiedQA: Crossing Format Boundaries With a Single QA System (2020)](https://arxiv.org/pdf/2005.00700.pdf)| √ |
| [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (2020)](https://arxiv.org/pdf/1910.10683.pdf)||
| [Open-Domain Question Answering (2020)](https://www.aclweb.org/anthology/2020.acl-tutorials.8.pdf) ||
| [Question Answering over Curated and Open Web Sources (2020)](https://dl.acm.org/doi/pdf/10.1145/3397271.3401421?casa_token=6ej4WcOKjr0AAAAA:Ex7k-Se8DlJLHLcgSMEvvGrwOEIHjVYUAD3_Hhv2r9l_oopOsuwZQRUAzPVy07cavnAwltwoOufXDw) | √ |
| [TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection (2019)](https://arxiv.org/pdf/1911.04118.pdf)||
| [DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs (2019)](https://arxiv.org/pdf/1903.00161.pdf)||
| [A survey on question answering systems over linked data and documents (2019)](https://link.springer.com/content/pdf/10.1007/s10844-019-00584-7.pdf) |   |
| [Reading Wikipedia to Answer Open-Domain Questions (2017)](https://arxiv.org/pdf/1704.00051.pdf)| |


## Datasets
| Datasets  | Des  |
|---|---|
| [Natural Questions: A Benchmark for Question Answering Research (2019)](https://www.aclweb.org/anthology/Q19-1026.pdf) ||
| [Tencent AI Lab Embedding Corpus for Chinese Words and Phrases (2018)](https://ai.tencent.com/ailab/nlp/en/embedding.html)||
| [SQuAD: 100,000+ Questions for Machine Comprehension of Text (2016)](https://arxiv.org/pdf/1606.05250.pdf)||
